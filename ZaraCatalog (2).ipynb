{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3843e431-cdbb-45a5-9136-4e833f5fe08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "asession = AsyncHTMLSession()\n",
    "import json\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef04c56-7196-40cc-85c7-97d5085638a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching data through API request  \n",
    "r = await asession.get('https://www.zara.com/in/en/categories?ajax=true')\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981b497-2e6b-4934-be80-23176c0bf741",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_string = r.text\n",
    "data = json.loads(json_string)\n",
    "# List to collect all the category codes\n",
    "my_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a0afc-109a-4a77-9b06-3d3eea330dd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# creating urls for each page of the site\n",
    "def invoke(id,name,sec_name,seo_id,extra):\n",
    "    url = f\"https://www.zara.com/in/en/category/{id}/products?regionGroupId=69&ajax=true\"\n",
    "    my_list.append([id,name,sec_name,seo_id, extra,url])\n",
    "\n",
    "\n",
    "cat = data['categories']    \n",
    "for i in cat:\n",
    "    extra = 0\n",
    "    # 4 main sc_1 (WOMAN, MAN, KIDS, BEAUTY)\n",
    "    sc_1 = i['subcategories']\n",
    "    for j in sc_1:\n",
    "        # BEAUTY has only one sub-section\n",
    "        id =j['id']\n",
    "        name = j['name']\n",
    "        sec_name = j.get(\"sectionName\")\n",
    "        seo_id = j.get('seo', {}).get('seoCategoryId', None)\n",
    "        if(i['name'] == 'BEAUTY'):\n",
    "            invoke(id,name,sec_name,seo_id, extra)\n",
    "        elif (j['subcategories'] != []):\n",
    "            sc_2 = j['subcategories']\n",
    "            for k in sc_2:\n",
    "                # MAN, WOMAN last subs\n",
    "                id = k['id']\n",
    "                m_name = k['name']\n",
    "                seo_id = k.get('seo', {}).get('seoCategoryId', None)\n",
    "                invoke(id,m_name,sec_name,seo_id,extra)\n",
    "                if (k['subcategories'] != []):\n",
    "                    # KIDS collection subcategories\n",
    "                    sc_3 = k['subcategories']\n",
    "                    for k_sc in sc_3:                                        \n",
    "                        id = k_sc['id']\n",
    "                        name = k_sc['name']\n",
    "                        seo_id = k_sc.get('seo', {}).get('seoCategoryId', None)\n",
    "                        invoke(id,name,sec_name,seo_id,extra=m_name)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c9014-19c8-477e-b65f-4bebb0664c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(my_list)\n",
    "df = pd.DataFrame(my_list)\n",
    "df.to_csv('zara_code_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de901e92-93f6-4506-97a7-24242ddaf143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#improve\n",
    "regex = r'\\+ Info|JOIN LIFE|GIFT CARD|info'\n",
    "for i in my_list:\n",
    "    for j in i:\n",
    "        if isinstance(j, str) and bool(re.search(regex, str(j), re.IGNORECASE)):\n",
    "            my_list.remove(i)\n",
    "\n",
    "print(len(my_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b02b3d-644c-4e7f-8ac4-482d67590cfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sections = ['WOMAN', 'MAN', 'KID', 'BEAUTY']\n",
    "product_list = []\n",
    "links = []\n",
    "\n",
    "for i in sections:\n",
    "    print(i)\n",
    "    for j in my_list:        \n",
    "        if i == j[2]:\n",
    "            val = j[5]\n",
    "            c = j[0]\n",
    "            # Collecting all responses of the urls\n",
    "            r = await asession.get(val)\n",
    "            print(r.status_code)\n",
    "            try:\n",
    "                r.raise_for_status()  # Raise an error for bad HTTP responses\n",
    "                json_string = r.text\n",
    "                data = json.loads(json_string)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"HTTP request error: {e}\")\n",
    "            product_list.append(data)\n",
    "            links.append(c)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834b79e-8a43-4982-9e1d-2b737afd1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "output_directory = \"C:\\Json files of zara 1200+responses\"\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Save each JSON object in a separate file\n",
    "for i, json_data in enumerate(product_list):\n",
    "    file_path = os.path.join(output_directory, f'zarajson_{i}_{links[i]}.json')\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(json_data, json_file, indent=2)\n",
    "\n",
    "print(f\"All {len(product_list)} JSON files have been saved to {output_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64cfc48-1377-4194-99e4-f83a9ed280a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def call(i):\n",
    "    file_path = f\"C:\\Json files of zara 1200+responses\\zarajson_{i}_{links[i]}.json\"\n",
    "    \n",
    "    # Read the JSON file\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        response = json.load(json_file)    \n",
    "    # Process the JSON data as needed\n",
    "    print(f\"Processing {file_path}:\")\n",
    "    return response\n",
    "\n",
    "# parsing every response and then storing them into individual files\n",
    "def extract_info_recursive_generator(data):\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            yield from extract_info_recursive_generator(item)\n",
    "    elif isinstance(data, dict):\n",
    "        xmedia = data.get(\"commercialComponents\", {})\n",
    "        for entry in xmedia:\n",
    "            yield {\n",
    "                \"name\": entry.get(\"name\"),\n",
    "                \"description\": entry.get(\"description\"),\n",
    "                \"price\": entry.get(\"price\"),\n",
    "                \"sectionName\": entry.get(\"sectionName\"),\n",
    "                \"familyName\": entry.get(\"familyName\"),\n",
    "                \"subfamilyName\": entry.get(\"subfamilyName\"),\n",
    "                \"id\": entry.get(\"detail\", {}).get(\"colors\", [])[0].get(\"id\"),\n",
    "                \"productId\": entry.get(\"detail\", {}).get(\"colors\", [])[0].get(\"productId\"),\n",
    "                \"color\": entry.get(\"detail\", {}).get(\"colors\", [])[0].get(\"name\")\n",
    "            }\n",
    "\n",
    "        for value in data.values():\n",
    "            yield from extract_info_recursive_generator(value)\n",
    "\n",
    "for i in range(1194):\n",
    "    data = call(i)\n",
    "    extracted_info_generator = extract_info_recursive_generator(data)\n",
    "    extracted_info_list = list(extracted_info_generator)\n",
    "\n",
    "    output_directory = \"C:\\Zara product details\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Save the extracted information to a separate JSON file for each input file\n",
    "    output_file_path = os.path.join(output_directory, f\"parsed_output_{i}.json\")\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(extracted_info_list, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8888d63a-d51c-433c-b3b9-8d03fa1bee80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Processing every output file and combining all of it into one single large dataframe\n",
    "result_df = pd.DataFrame()\n",
    "for i in range(1194):        \n",
    "    # Specify the directory where the JSON files are saved\n",
    "    file_path = f\"C:\\Zara product details\\parsed_output_{i}.json\"\n",
    "    # Read the JSON file\n",
    "    with open(file_path, 'r') as json_file:\n",
    "        response = json.load(json_file)    \n",
    "    # Process the JSON data as needed\n",
    "    print(f\"Processing {file_path}:\")\n",
    "    df = pd.DataFrame(response)\n",
    "    df['category_id'] = links[i]\n",
    "    result_df = pd.concat([result_df, df], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aedf4d9-0863-4c9c-a09a-ecd7389dca79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Total rows = 102147\n",
    "result_df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b33f09-69cb-4a00-8d34-237769e0e180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
